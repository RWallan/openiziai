import json
import random
from datetime import datetime
from math import ceil
from pathlib import Path
from typing import Any, Optional

import trio
from openai import OpenAI
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    PrivateAttr,
)

from openiziai.schemas import DataDict
from openiziai.task import Task
from openiziai.utils import exponential_backoff


class TrainDataTool(BaseModel):
    """Cria dados preparados para fine tuning.

    Cria os dados para construir um fine tuning de um modelo LLM centrado
    em uma task contendo: backstory, role e o goal.

    :param client: Client da OpenAI.
    :param data: Dados utilizados para construir o dado e treino.
    :param task: Descrição da task que o modelo deverá executar.
    :param model: Modelo GPT usado para criar os dados para treino. Padrão gpt-3.5-turbo-125.
    :type client: OpenAI
    :type data: DataDict
    :type task: Task
    :type model: str
    """  # noqa

    client: OpenAI = Field(default=None, description='Client da OpenAI.')
    data: DataDict = Field(
        default=None,
        description='Dados utilizados para construir o dado de treino.',
    )
    task: Task = Field(
        default=None,
        description='Descrição da task que o modelo deverá executar.',
    )
    model: str = Field(
        default='gpt-3.5-turbo-125',
        description='Modelo GPT que criará o dado de fine tuning.',
    )

    _n_examples: int = PrivateAttr(default=None)
    _n_batch: int = PrivateAttr(default=None)
    _root: Path = PrivateAttr(default_factory=Path.cwd)
    _train_data_dir: Path
    _template: str

    model_config = ConfigDict(arbitrary_types_allowed=True)

    def __init__(self, **data: Any) -> None:
        """Cria um novo modelo analisando e validando o input.

        Raises [ValidationError][pydantic_core.ValidationError]
        se os inputs não podem ser validados para forma um modelo válido.
        """
        super().__init__(**data)
        self._template = """You are generating data which will be used to train a machine learning model.
        To describe how the model will be trained, you will be given a high-level description in a dict with:
        ```{{'backstory': $backstory_goes_here, 'role': $role_goes_here, 'goal': $goal_goes_here, 'data': $data_goes_here}}
        From that, you will generate data samples, each with a prompt/response pair. You must return the sample in jsonl format with the exact informations:
        ```{{'prompt': $prompt_goes_here, 'response': $response_goes_here}}```
        Only one prompt/response pair should be generated by turn. For each turn, make the example slightly more complex than the last, while ensuring diversity.
        Here is the high-level description to generate the samples:
        {description}
        """  # noqa

        self._train_data_dir = self._create_dir_if_not_exist(
            self._root / 'data' / 'train'
        )

    @property
    def train_data_dir(self) -> str:
        return str(self._train_data_dir)

    @staticmethod
    def _create_dir_if_not_exist(dir: Path) -> Path:
        dir.mkdir(parents=True, exist_ok=True)

        return dir

    @exponential_backoff()
    async def create_examples(  # noqa
        self,
        n_examples: int,
        temperature: float,
        max_tokens: int,
        max_context_length: int,
        sender: trio.abc.SendChannel[list[dict[str, str]]],
    ) -> None:
        description = dict(
            backstory=self.task.backstory,
            role=self.task.role,
            goal=self.task.goal,
            **self.data,
        )
        prompt = self._template.format(description=description)

        short_system = {'role': 'system', 'content': self.task.short_backstory}

        examples = []

        for _ in range(n_examples):
            context = (
                [
                    {'role': 'assistant', 'content': example}
                    for example in examples
                ]
                if len(examples) <= max_context_length
                else [
                    {'role': 'assistant', 'content': example}
                    for example in random.sample(examples, 8)
                ]
            )

            messages = [
                {
                    'role': 'system',
                    'content': prompt,
                }
            ] + context

            result = self.client.chat.completions.create(
                model=self.model,
                messages=messages,  # pyright: ignore
                temperature=temperature,
                max_tokens=max_tokens,
            )

            _response = result.choices[0].message.content
            if not _response:
                print('Algo de errado aconteceu ao criar o prompt.')
                continue
            response = json.loads(_response)
            example = {
                'messages': [
                    short_system,
                    {'role': 'user', 'content': response.get('prompt')},
                    {
                        'role': 'assistant',
                        'content': response.get('response'),
                    },
                ]
            }

            examples.append(example)
        async with sender:
            await sender.send(examples)

    async def create_train_file(self, receiver: trio.abc.ReceiveChannel):
        self._file = (
            self._train_data_dir
            / f'train_data_{datetime.now().strftime("%Y%m%d")}.jsonl'
        )

        if self._file.exists():
            self._file.unlink()

        async with receiver:
            async for results in receiver:
                async with await trio.open_file(
                    self._file, 'a', encoding='utf-8'
                ) as file:
                    for result in results:
                        await file.write(json.dumps(result) + '\n')

    async def create_train_data(  # noqa
        self,
        n_examples: int,
        n_batch: int,
        temperature: float = 0.5,
        max_tokens: int = 1000,
        max_context_length: int = 8,
    ) -> str:
        """Cria os dados de treino.

        :param n_examples: Número de exemplos que devem ser criados.
        :param n_batch: Número de batches para ser feitos concorrentes.
        :param temperature: Nível de criatividade do modelo ao criar os exemplos. Padrão 0.5.
        :param max_tokens: Limite de tokens da resposta do modelo. Padrão 1000.
        :max_context_length: Limite de exemplos utilizados para contexto.
        :type n_examples: int
        :type n_batch: int
        :type temperature: float
        :type max_tokens: int
        :type max_context_length: int
        :return: Nome do arquivo JSONL salvo.
        :rtype: str
        """  # noqa
        sender, receiver = trio.open_memory_channel(0)
        self._n_batch = n_batch
        _n_examples = ceil(n_examples / self._n_batch)
        self._n_examples = _n_examples * self._n_batch

        async with trio.open_nursery() as nursery:
            async with sender, receiver:
                for _ in range(self._n_batch):
                    nursery.start_soon(
                        self.create_examples,
                        _n_examples,
                        temperature,
                        max_tokens,
                        max_context_length,
                        sender.clone(),
                    )
                    nursery.start_soon(
                        self.create_train_file,
                        receiver.clone(),
                    )

        return str(self._file)
